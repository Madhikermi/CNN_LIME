{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Classifications of in-vivo Gastral Images - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from shutil import copyfile\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "We need to specify a set of \"constants\" (Python does not have constants, but it really *should* be constants ;-)) that depend on the data location of the machine that runs the code.\n",
    "Note that you can get the data from https://rdm.inesctec.pt/dataset/nis-2018-003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = '/Users/timotheuskampik/Downloads/labels.txt' # Path to labels file\n",
    "data_path = '/Users/timotheuskampik/Downloads/dataset/Set_1/A' # Path to data folder\n",
    "working_path = '/Users/timotheuskampik/Downloads/training' # Path for (to-be-prepared) training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "First, we need to prepare our data. We split data and labels into train and validation sets (randomly assigned) and, for each set, separate *bleeding* and *non-bleeding* images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 3295; label length: 3295\n"
     ]
    }
   ],
   "source": [
    "training_path = f'{working_path}/training'\n",
    "validation_path = f'{working_path}/validation'\n",
    "validation_split = 0.1\n",
    "\n",
    "# read in labels\n",
    "labels_file = open(label_path, 'r')\n",
    "labels = labels_file.read().splitlines()\n",
    "\n",
    "# get data set length\n",
    "_, _, files = next(os.walk(data_path))\n",
    "data_length = len(files)\n",
    "\n",
    "print(f'Data length: {data_length}; label length: {len(labels)}')\n",
    "\n",
    "# create directory structure\n",
    "if not os.path.exists(training_path):\n",
    "    os.makedirs(training_path)\n",
    "if not os.path.exists(validation_path):\n",
    "    os.makedirs(validation_path)\n",
    "\n",
    "if not os.path.exists(f'{training_path}/bleeding'):\n",
    "    os.makedirs(f'{training_path}/bleeding')\n",
    "if not os.path.exists(f'{training_path}/nonbleeding'):\n",
    "    os.makedirs(f'{training_path}/nonbleeding')\n",
    "if not os.path.exists(f'{validation_path}/bleeding'):\n",
    "    os.makedirs(f'{validation_path}/bleeding')\n",
    "if not os.path.exists(f'{validation_path}/nonbleeding'):\n",
    "    os.makedirs(f'{validation_path}/nonbleeding')\n",
    "\n",
    "# copy files into directory structure\n",
    "index = 0\n",
    "for _, _, files in os.walk(data_path):\n",
    "    for file in files:\n",
    "        base_path = training_path\n",
    "        if random.uniform(0, 1) < validation_split: # assign pseudo-randomly to training or validation set\n",
    "            base_path = validation_path\n",
    "        if labels[index] == '0':\n",
    "            copyfile(os.path.join(data_path, file), f'{base_path}/nonbleeding/{file}') \n",
    "        else:\n",
    "            copyfile(os.path.join(data_path, file), f'{base_path}/bleeding/{file}')\n",
    "        index = index + 1\n",
    "\n",
    "_, _, files = next(os.walk(data_path))\n",
    "training_data_length = len(files)\n",
    "\n",
    "_, _, files = next(os.walk(data_path))\n",
    "validation_data_length = len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now, we save the model and save it to disk. For this, we use this Keras code as the blue print: https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3144: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Found 2959 images belonging to 2 classes.\n",
      "Found 336 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "  6/205 [..............................] - ETA: 1:50 - loss: 0.6960 - acc: 0.5729"
     ]
    }
   ],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "nb_train_samples = training_data_length\n",
    "nb_validation_samples = validation_data_length\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_path,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_path,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
